Secure AI MNIST – CNN Classification and Adversarial Robustness Report
==================================================================

(Use this text as a starting point for your final report. Replace all values in [square brackets]
with the actual numbers from your own runs, taken from the JSON metrics files produced by the code.)

Student name : [Your Name]
Roll number  : [Your Roll]
Date         : [Date]

------------------------------------------------------------------
0. Mapping to Assignment Deliverables
------------------------------------------------------------------
(a) Link of GitHub repository of CNN implementation (MNIST)  -> Section 1
(b) Performance metrics of classification (clean dataset)     -> Section 2
(c) Threat model of the implementation (STRIDE)               -> Section 3
(d) Link of repo/code for generating adversarial dataset      -> Section 1
(e) Performance metrics with adversarial dataset              -> Section 4 and 5
(f) Conclusion and observations                               -> Section 6

------------------------------------------------------------------
1. GitHub Repositories
------------------------------------------------------------------

1.1 CNN implementation for MNIST classification
    Repository URL: [https://github.com/<your-username>/<your-mnist-cnn-repo>]

    Contents (suggested layout):
    - /code/baseline_cnn.py               : trains the clean CNN model on MNIST
    - /code/secure_ai_mnist_adv.py        : code for poisoning, FGSM generation,
                                             adversarial evaluation and robust training
    - /secure_ai_outputs/images/          : saved plots (loss/accuracy curves,
                                             confusion matrices, example images)
    - /secure_ai_outputs/models/          : saved model weights (baseline, poisoned,
                                             adversarially trained)
    - /secure_ai_outputs/metrics/         : JSON files with accuracy, loss,
                                             confusion matrices and inference times.

1.2 Adversarial dataset generation
    Repository (or subfolder): [https://github.com/<your-username>/<your-mnist-cnn-repo>]
    (If you use a single repo, mention that this same repo also contains the adversarial
    dataset generation code.)

    Key scripts:
    - poison_corner_trigger(): injects a fixed corner patch in a fraction of training
      images and flips their labels to the chosen target class (backdoor / trigger).
    - build_fgsm_attack(): uses the ART FastGradientMethod attack to generate FGSM
      adversarial examples from the trained CNN.
    - save_adv_examples(): saves a sample of adversarial images for visualization
      and creates metrics JSON files.

------------------------------------------------------------------
2. Performance Metrics on Clean MNIST (Baseline CNN)
------------------------------------------------------------------

2.1 Model architecture

The baseline CNN is a simple 2-layer convolutional network implemented in Keras:

- Input: 28 x 28 grayscale image, rescaled to [0, 1], shape (28, 28, 1)
- Conv2D (32 filters, 3x3, ReLU)
- Conv2D (64 filters, 3x3, ReLU)
- MaxPooling2D (2x2)
- Dropout (rate 0.25)
- Flatten
- Dense (128 units, ReLU)
- Dropout (rate 0.5)
- Dense (10 units, softmax)

Training setup (typical values; keep them consistent with your code):
- Optimizer  : Adam
- Loss       : categorical_crossentropy
- Batch size : [128]
- Epochs     : [10]
- Dataset    : MNIST (60k train, 10k test), with pixel values /255.0.

2.2 Test performance on clean MNIST

(Values to be taken from metrics JSON, e.g.
secure_ai_outputs/metrics/baseline_clean_metrics.json.)

- Test accuracy       : [ACC_CLEAN]%
- Test loss           : [LOSS_CLEAN]
- Inference time      : [TIME_CLEAN_MS] ms per image (averaged over test set)
- Confusion matrix    : stored as image
    -> File: secure_ai_outputs/images/confusion_baseline_clean.png
       (rows = true labels, columns = predicted labels)

Qualitative observation:
- The confusion matrix is close to diagonal, showing that the CNN learns
  a strong decision boundary on MNIST.
- Most errors usually occur between visually similar digits, e.g.
  3 vs 5, 4 vs 9, and 5 vs 8. (Check and update this sentence based on
  your own confusion matrix.)

------------------------------------------------------------------
3. Threat Model and STRIDE-based Threat Modeling
------------------------------------------------------------------

3.1 System overview

We consider the following components:

- Data source: the MNIST dataset and any additional images used for training.
- Training pipeline: loading data, preprocessing, training the CNN, saving weights.
- Adversarial data generation: poisoning (corner trigger) and FGSM attacks.
- Inference interface: code that loads a trained model and outputs predictions.

Primary security goal:
- Maintain high classification accuracy for benign inputs while limiting accuracy
  degradation under realistic adversarial attacks (poisoning and evasion).
- Prevent unintended or malicious manipulation of the model, training data,
  and inference results.

3.2 Adversary assumptions

- The attacker can:
  * Evasion setting: Submit crafted images to the trained model. In our experiments,
    this corresponds to FGSM adversarial examples with L-infinity perturbation
    bounded by epsilon = [0.3] on normalized pixel range [0, 1].
  * Poisoning/backdoor setting: Inject or modify a small fraction of training
    samples before training. In our implementation this is modelled by adding
    a fixed white corner patch to a subset (e.g. [5]% or [10]%) of training
    images and forcing their label to the target class (e.g. digit 0).
  * Model theft / reverse engineering: Download model weights from a public repo
    (if made public) and perform offline analysis.
- The attacker cannot:
  * Modify the code after deployment without compromising the host system.
  * Break standard cryptographic primitives (we assume OS and file-system integrity).
  * Directly control the training labels in the original MNIST dataset.

3.3 STRIDE analysis

We analyse threats for the system using the STRIDE framework:

S – Spoofing identity
---------------------
- Threats:
  * If the model is exposed as a network service, an attacker could spoof a
    legitimate client's identity and send adversarial queries, bypassing any
    simplistic IP-based access control.
  * A malicious actor could upload a fake repo that pretends to be the official
    implementation (e.g., typo-squatting on GitHub).
- Mitigations:
  * Use proper authentication/authorization when deploying a remote inference API
    (tokens, OAuth, etc.).
  * Verify repository ownership (e.g., through institution-controlled GitHub
    organization accounts).
  * Use HTTPS and signed releases for distributed artifacts.

T – Tampering
-------------
- Threats:
  * Poisoning the training data by adding or modifying samples (e.g., injecting
    the corner trigger backdoor).
  * Tampering with saved model checkpoints to weaken robustness or to plant
    malicious behavior.
  * Altering evaluation scripts to misreport accuracy or hide adversarial failures.
- Mitigations:
  * Keep a read-only copy of the original dataset; validate checksums.
  * Use version control and integrity checks (e.g., hashes) for model files.
  * Restrict write permissions on training and deployment machines.
  * Include unit tests / sanity checks (e.g. visual spot-check of training samples,
    distribution checks) to detect unexpected triggers or label flips.

R – Repudiation
---------------
- Threats:
  * An attacker (or even an insider) might deny having submitted poisoned data
    or adversarial inputs.
  * A collaborator might deny having changed hyperparameters or checkpoints that
    degraded robustness.
- Mitigations:
  * Enable logging for model training and inference: record who triggered training,
    commit hashes, configuration files, and summary metrics.
  * Store logs with timestamps and, where relevant, cryptographic signatures so
    that actions can be audited later.
  * Use issue trackers / pull requests to document code and data changes.

I – Information disclosure
--------------------------
- Threats:
  * Publishing model weights may leak information about training data if the
    data were sensitive (membership inference or model inversion attacks).
    (MNIST itself is public and not sensitive, but the same pipeline could be
    used on private data.)
  * Exposed metrics or misconfigured debugging interfaces might reveal more
    about the model than intended.
- Mitigations:
  * If using sensitive data, consider differential privacy or training with
    strict access control.
  * Avoid exposing internal debugging endpoints in production deployments.
  * Limit the detail of error messages and stack traces in any public interface.

D – Denial of Service (DoS)
---------------------------
- Threats:
  * Flooding the inference API with a large number of requests (including
    adversarial examples) to consume CPU/GPU resources and make the service
    unavailable for legitimate users.
  * Sending extremely large or malformed inputs that cause the model pipeline
    to crash or hang.
- Mitigations:
  * Rate-limit clients and apply quotas per user/API key.
  * Validate input size, type, and format before passing data to the model.
  * Deploy monitoring and autoscaling solutions (for larger systems) to handle
    spikes in traffic.

E – Elevation of privilege
--------------------------
- Threats:
  * An attacker with limited access (e.g. only inference API) might find a way
    to run arbitrary code or gain shell access on the server, then modify the
    model or training pipeline.
  * Misconfigured CI/CD or deployment scripts might allow untrusted code to
    be executed with high privileges.
- Mitigations:
  * Run model services under least-privilege OS accounts.
  * Isolate training and inference processes using containers or VMs.
  * Regularly patch the OS, ML libraries, and dependencies.
  * Carefully review CI/CD pipelines and ensure only trusted code is executed.

Summary:
--------
The primary technical attacks studied in this project (data poisoning with
triggers and FGSM adversarial examples) fall mainly under the Tampering and
Denial-of-Service categories, but their impact is also relevant to Integrity
(the correctness of predictions) and Availability. The STRIDE analysis shows
that, if the same model and code were deployed as a real service, additional
security measures would be required around identity, logging, and privilege
separation.

------------------------------------------------------------------
4. Adversarial Dataset Generation
------------------------------------------------------------------

4.1 Method 1 – Corner-trigger data poisoning (backdoor)

Goal:
- Force the CNN to associate a small fixed pattern (trigger) with a chosen
  target label, so that at test time any image containing the trigger is
  misclassified as the target class even if the rest of the image looks like
  a different digit.

Implementation details:
- Trigger pattern: a small white square in the bottom-right corner of the
  28x28 image (e.g., 3x3 patch with pixel value 1.0).
- Target label: [TARGET_CLASS] (for example, digit 0).
- Poisoning fraction: [POISON_FRACTION]% of the original training images are
  selected at random:
  * For each selected image, the trigger is overlaid.
  * The label is overwritten with the target class.
- Training:
  * The CNN is re-trained on the poisoned training set, starting from scratch
    or fine-tuning from the clean baseline model.
  * Metrics are saved separately as
      secure_ai_outputs/metrics/poisoned_clean_metrics.json
      secure_ai_outputs/metrics/poisoned_triggered_metrics.json

4.2 Method 2 – FGSM adversarial examples (evasion attack)

Goal:
- Generate minimally perturbed images that look normal to humans but cause
  misclassification by the CNN.

Implementation details:
- Library: Adversarial Robustness Toolbox (ART) – FastGradientMethod.
- Attack parameters (typical):
  * Norm: L_infinity
  * Epsilon: [0.3]
  * Targeted or untargeted: [untargeted]
  * Number of iterations: 1 (FGSM is a single-step method)
- Workflow:
  * Wrap the trained Keras CNN model in an ART KerasClassifier.
  * Call attack.generate(x=test_images) to obtain adversarial samples.
  * Evaluate the CNN on the adversarial test set and save metrics as
      secure_ai_outputs/metrics/baseline_fgsm_metrics.json
  * Save example adversarial images in
      secure_ai_outputs/images/fgsm_examples.png

------------------------------------------------------------------
5. Performance Metrics with Adversarial Data
------------------------------------------------------------------

5.1 Baseline CNN under poisoning and adversarial attacks

Use the metrics JSON files produced by your code to fill the following:

- Poisoned training, evaluated on clean test data:
    * Accuracy    : [ACC_POISONED_CLEAN]%
    * Loss        : [LOSS_POISONED_CLEAN]
    * Inference t.: [TIME_POISONED_CLEAN_MS] ms per image
    * Confusion matrix image:
        secure_ai_outputs/images/confusion_poisoned_clean.png

- Poisoned training, evaluated on triggered test data:
    * Accuracy    : [ACC_POISONED_TRIGGER]%
    * Loss        : [LOSS_POISONED_TRIGGER]
    * Inference t.: [TIME_POISONED_TRIGGER_MS] ms per image
    * Confusion matrix image:
        secure_ai_outputs/images/confusion_poisoned_trigger.png
    * Expected behaviour: accuracy on normal digits may remain high, but any
      image that contains the trigger patch tends to be predicted as the
      target class. This shows a successful backdoor.

- Cleanly trained baseline CNN, evaluated on FGSM adversarial test data:
    * Accuracy    : [ACC_BASELINE_FGSM]%
    * Loss        : [LOSS_BASELINE_FGSM]
    * Inference t.: [TIME_BASELINE_FGSM_MS] ms per image
    * Confusion matrix image:
        secure_ai_outputs/images/confusion_baseline_fgsm.png
    * Expected behaviour: accuracy drops sharply (often close to random,
      ~10%) even though the perturbations are visually small.

5.2 Robust model – Adversarial training with FGSM (Task 7)

Adversarial training procedure:
- Generate FGSM adversarial examples on the fly during training (or pre-generate
  them) and train the CNN on a mixture of clean and adversarial images.
- Typical ratio: 50% clean, 50% FGSM adversarial per batch.
- Keep the same architecture and optimizer as the baseline.

Metrics to fill (from metrics JSON, e.g.
secure_ai_outputs/metrics/robust_clean_metrics.json and
secure_ai_outputs/metrics/robust_fgsm_metrics.json):

- Robust model evaluated on clean test data:
    * Accuracy    : [ACC_ROBUST_CLEAN]%
    * Loss        : [LOSS_ROBUST_CLEAN]
    * Inference t.: [TIME_ROBUST_CLEAN_MS] ms per image
    * Confusion matrix image:
        secure_ai_outputs/images/confusion_robust_clean.png

- Robust model evaluated on FGSM adversarial test data:
    * Accuracy    : [ACC_ROBUST_FGSM]%
    * Loss        : [LOSS_ROBUST_FGSM]
    * Inference t.: [TIME_ROBUST_FGSM_MS] ms per image
    * Confusion matrix image:
        secure_ai_outputs/images/confusion_robust_fgsm.png

Qualitative comparison:
- The robust model usually achieves slightly lower accuracy on clean data
  compared to the baseline CNN (e.g., 98% vs. 99%), because training becomes a
  harder optimization problem.
- However, accuracy on FGSM adversarial data improves dramatically (e.g.,
  from near-random ~10% up to 60–90%, depending on epsilon and training setup).
  This demonstrates the classical trade-off between clean accuracy and
  adversarial robustness.

------------------------------------------------------------------
6. Conclusion and Observations
------------------------------------------------------------------

(You can keep this structure and tailor the bullet points to your actual
metrics and plots.)

- The baseline CNN achieves high accuracy on clean MNIST ([ACC_CLEAN]%), with
  a confusion matrix that is nearly diagonal and a low per-image inference
  time ([TIME_CLEAN_MS] ms). This confirms that even a simple CNN can solve
  MNIST effectively.

- The corner-trigger poisoning experiment shows that a small fraction of
  poisoned training data ([POISON_FRACTION]%) is sufficient to implant a
  strong backdoor. The poisoned model continues to perform well on clean
  data but misclassifies most triggered inputs as the target class
  ([TARGET_CLASS]), illustrating the severity of data poisoning attacks.

- Under FGSM adversarial examples, the clean baseline CNN's accuracy drops
  sharply to [ACC_BASELINE_FGSM]%, despite the perturbations being almost
  imperceptible to humans. The confusion matrix for adversarial data is
  much less diagonal, demonstrating loss of robustness.

- Adversarial training with FGSM significantly improves robustness: the
  adversarially trained model maintains a higher accuracy on FGSM inputs
  ([ACC_ROBUST_FGSM]%) at the cost of a small reduction in clean accuracy
  ([ACC_ROBUST_CLEAN]% vs [ACC_CLEAN]%).

- From a STRIDE perspective, the main technical risks in this project are
  associated with Tampering (poisoning and adversarial example generation)
  and Denial of Service (potential overload of inference). If the same
  pipeline were deployed in a real system, additional mitigations around
  Spoofing, Repudiation, Information disclosure, and Elevation of privilege
  would be necessary.

- Overall, the experiments highlight a key lesson of secure machine learning:
  high test accuracy on clean data is not sufficient. Robustness against
  deliberate adversarial manipulation must be designed and evaluated explicitly,
  using both algorithmic defenses (such as adversarial training) and system
  security measures (such as those identified in the STRIDE analysis).

------------------------------------------------------------------
End of report.
------------------------------------------------------------------
